#!/bin/bash
#SBATCH -A uot193
#SBATCH --job-name="add"
#SBATCH --output="add.distr.out"
#SBATCH --partition=compute
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=128
#SBATCH --mem=249208M
#SBATCH --export=ALL 
#SBATCH --time=59

export HADOOP_CONF_DIR=/home/$USER/expansecluster
module load cpu/0.15.4 gcc/7.5.0 openjdk
SW=/expanse/lustre/projects/uot193/fegaras
export HADOOP_HOME=$SW/hadoop-3.2.2
export MYHADOOP_HOME=$SW/myhadoop
PATH="$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$MYHADOOP_HOME/bin:$PATH"

myhadoop-configure.sh -s /scratch/$USER/job_$SLURM_JOBID

start-dfs.sh
start-yarn.sh

hdfs dfs -mkdir -p /user/$USER
hdfs dfs -put $SW/M-matrix-large.txt /user/$USER/M-matrix-large.txt
hdfs dfs -put $SW/N-matrix-large.txt /user/$USER/N-matrix-large.txt
hadoop jar add.jar Add 200 300 /user/$USER/M-matrix-large.txt /user/$USER/N-matrix-large.txt /user/$USER/left_tmp /user/$USER/right_tmp /user/$USER/output
rm -f large-output.txt
hdfs dfs -tail /user/$USER/output/part-r-00000 >large-output.txt

stop-yarn.sh
stop-dfs.sh
myhadoop-cleanup.sh
