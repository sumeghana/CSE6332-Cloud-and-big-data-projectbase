# CSE6332-Cloud-and-big-data-projectbase
Collection of different problem sets in Hadoop mapreduce,PIG,HIVE and Apache Spark


# PROJECT-1
In this project, you are asked to do some simple statistical analysis using Map-Reduce. You will use a real dataset from the NYC Yellow Taxi Trip Data, described at kaggle.com. The data is located on Expanse in the directory `/expanse/lustre/projects/uot193/fegaras/tripdata/` (about 3GBs total). A small part of this data (the first 72 lines) is given in the file `tripdata-small.csv` inside project1. The first line of each file contains the CSV attribute names and should be skipped.

We are interested in the attributes `trip_distance` (attribute at position 5) and `total_amount` (last attribute) of this dataset. We first need to round off the trip distance to the nearest integer (e.g., 2.34 miles to 2 miles). Then, for each different rounded trip distance that is less than 200 miles, we want to calculate the average total amount paid. 
Hints:
- How to parse a text line and break it into strings? You may use the Java String method split.
- How to ignore the attribute description line (first line)? Before you parse, check the first character of the text line. If it's not numeric, ignore it.

# PROJECT-2
For this project, you are asked to implement block matrix addition in Map-Reduce. You will take two sparse matrices as input, convert them to block matrices, and then perform block matrix addition on them. The sum of two matrices M and N is the matrix R such that `Rij = Mij + Nij`. A sparse matrix is a dataset of triples `(i, j, v)`, where `i` and `j` are the indices and `v` is the matrix value at the indices `i` and `j`. A triple is constructed with the Java class `Triple`. 

A block matrix is a dataset of blocks. Each block is a dense Java matrix of size `rows * columns` constructed with the Java class `Block` (the values `rows` and `columns` are program arguments). A block matrix is stored in HDFS as a binary file (in `SequenceTextInputFormat`) of key-values, where the key is a pair of block coordinates `(ci, cj)` (constructed with the Java class `Pair`) and the value is a block. A matrix element `Mij` is stored inside the block with block coordinates `(i/rows, j/columns)` at the location `(i%rows, j%columns)` inside the block. The block matrix addition of M and N is done by finding blocks from M and N with the same block coordinates and by adding the blocks together using regular matrix addition in Java.

Your project is to convert two sparse matrices M and N which are read from files to block matrices and then add them using block matrix addition. First, you need to convert a sparse matrix to a block matrix using the following Map-Reduce pseudo-code (called twice, for M and for N):

# PROJECT-3
Write a Map-Reduce program that finds the connected components of any undirected graph and prints the size of these connected components. A connected component of a graph is a subgraph of the graph in which there is a path between any two vertices in the subgraph. For the above graph, there are two connected components: one 0,8,9 and another 1,2,3,4,5,6,7. Your program should print the sizes of these connected components: 3 and 7.

The following pseudo-code finds the connected components. It assigns a unique group number to each vertex (we are using the vertex ID as the initial group number), and for each graph edge between Vi and Vj, it changes the group number of these vertices to the minimum group number of Vi and Vj. That way, vertices connected together will eventually get the same minimum group number, which is the minimum vertex ID among all vertices in the connected component. First you need a class to represent a vertex:

# PROJECT-4
For this project, you need to implement block matrix addition using Spark in Scala. Do not use Hadoop Map-Reduce. You will take two sparse matrices as input, convert them to block matrices, and then perform block matrix addition on them. A sparse matrix is a dataset of triples `(i, j, v)`, where `i` and `j` are the indices (of type `Int` in Scala) and `v` is that matrix value (of type `Double` in Scala) at the indices `i` and `j`. A block matrix is a Spark RDD of blocks of type `RDD[((Int, Int), Block)]` in Scala, where the two `Int` are the block coordinates. A Block has type `Array[Double]` in Scala and has size `rows * columns`, where `rows` and `columns` are arguments in the main program. A matrix element `Mij` is stored inside the block with block coordinates `(i/rows, j/columns)` at the location `(i%rows)*columns + (j%columns)` inside the block. The block matrix addition of M and N is done by finding blocks from M and N with the same block coordinates and by adding the blocks together using regular matrix addition in Scala.

Your project is to convert two sparse matrices M and N which are read from files to block matrices using the Scala function `createBlockMatrix` and then to add them using block matrix addition. In your Scala main program, `args(0)` is the number of rows, `args(1)` is the number of columns, `args(2)` is the first input matrix M, and `args(3)` is the second input matrix N. You should only print to the output the block that you derive from matrix addition with block coordinates `(1, 2)`. Like in Project-2, there are two small sparse matrices of sizes 35*48 in the files `M-matrix-small.txt` and `N-matrix-small.txt` for testing in local mode using `rows=8` and `columns=6`. Your block with coordinates `(1, 2)` must be similar to that in `small-solution.txt`. Then, there are 2 moderate-sized matrices 5000*10000 in the files `M-matrix-large.txt` and `N-matrix-large.txt` for testing in distributed mode (located on Expanse) using `rows=200` and `columns=300`. So each block matrix will have 5*10 blocks. Your block with coordinates `(1, 2)` must be similar to that in `large-solution.txt`.

# PROJECT-5
Like in Project-3, your task is to write a Spark program that finds the connected components of any undirected graph and prints the size of these connected components. A connected component of a graph is a subgraph of the graph in which there is a path between any two vertices in the subgraph. As you can see in the incomplete `Graph.scala` program, the variable `graph` has type `RDD[(Long, Long, List[Long])]`. That is, it is a dataset of vertices, where each vertex is a triple `(group, id, adj)`, where `id` is the vertex id, `group` is the group id (initially equal to `id`), and `adj` is the list of outgoing neighbors. For example, from the input line `8,5,6,7`, you need to return the tuple `(8, 8, List(5, 6, 7))`. Then, during the for-loop, you need to construct a dataset which, for each vertex, generates group candidates. For example, from the vertex `(4, 8, List(5, 6, 7))` you generate the group candidates `(8, 4)`, `(5, 4)`, `(6, 4)`, and `(7, 4)`, that is, 8 can be in group 4, 5 can be in group 4, etc. Then, for each vertex id, you select the minimum group candidate. This will be the new group for this vertex at this iteration. These new groups are stored in the variable `groups`. Then, you need to reconstruct the graph with the new groups so that your program can do more iterations (there are 5 iterations). After the loop, you print the sizes of each group.

# PROJECT-6
You are asked to do Project-1 (NYC Yellow Taxi Trip Data) using Apache Pig. Like in Project-1, we are interested in the attributes `trip_distance` (attribute at position 5) and `total_amount` (last attribute) of this dataset. We first need to round off the trip distance to integers (e.g., 2.34 miles to 2 miles). Then, for each different rounded trip distance that is less than 200 miles, we want to calculate the average total amount paid. You need to print the results to the output using `dump`. In your Pig script, you can access the path of the trip dataset as '$T'. That is, you can use `LOAD '$T' USING ...` to load this dataset.
Steps:
1. Read the input as CSV without schema. If the first column of CSV is `VentorID`, then skip it. Otherwise, extract the `trip_distance` and the `total_amount` and round them up and convert them to the appropriate types. 
2. Construct the groups and calculate the averages in the groups.

# PROJECT-7
You are asked to do Project-1 (NYC Yellow Taxi Trip Data) using Spark SQL. Like in Project-1, we are interested in the attributes `trip_distance` (attribute at position 5) and `total_amount` (last attribute) of
